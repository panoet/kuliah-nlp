{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483dd18-5509-4b0f-b808-5e5207d66bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ADOPTED FROM https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7328e6-f0c6-4189-8c32-458d9a3041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fd5b8-e49e-4c46-a145-2ee03af29d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE DAN BUAT LIST\n",
    "filepath = 'dataset_vaksin.csv'\n",
    "column_names = [\"Tweet\", \"Label\"]\n",
    "\n",
    "df = read_csv(filepath, header=0, sep=',', names=column_names)\n",
    "messages = df.Tweet.to_list()\n",
    "sentiments_ = df.Label.to_list()\n",
    "\n",
    "sentiments = [1 if sentiment =='Setuju Vaksin' else 0 for sentiment in sentiments_]\n",
    "\n",
    "#print(df)\n",
    "#print('----------------------------------------------------------------')\n",
    "print(messages[0:10])\n",
    "#print('----------------------------------------------------------------')\n",
    "print(sentiments[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e202a3-3bb7-47ea-bc1e-f737ce82f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS\n",
    "import re\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - removes any single character tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed\n",
    "    Returns\n",
    "    -------\n",
    "        text: The preprocessed text\n",
    "    \"\"\" \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    return text\n",
    "        \n",
    "# Process for all messages\n",
    "preprocessed = [preprocess(message) for message in tqdm(messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8292df-eef9-481b-a132-7146a66cd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logger.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abdc79-b9aa-49cd-ba52-c3b26eaf6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VOCAB\n",
    "def create_vocab(messages, show_graph=False):\n",
    "    corpus = []\n",
    "    for message in tqdm(messages, desc=\"Tokenizing\"):\n",
    "        tokens = tokenize_text(message, 3) # Use option 3\n",
    "        corpus.extend(tokens)\n",
    "##    logger.info(\"The number of all words: {}\".format(len(corpus)))\n",
    "\n",
    "    # Create Counter\n",
    "    counts = Counter(corpus)\n",
    "##    logger.info(\"The number of unique words: {}\".format(len(counts)))\n",
    "\n",
    "    # Create BoW\n",
    "    bow = sorted(counts, key=counts.get, reverse=True)\n",
    "##    logger.info(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
    "\n",
    "    # Indexing vocabrary, starting from 1.\n",
    "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    if show_graph:\n",
    "        from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "        # Generate Word Cloud image\n",
    "        text = \" \".join(corpus)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update([\"will\", \"report\", \"reporting\", \"market\", \"stock\", \"share\"])\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(text)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Show most frequent words in a bar graph\n",
    "        most = counts.most_common()[:80]\n",
    "        x, y = [], []\n",
    "        for word, count in most:\n",
    "            if word not in stopwords:\n",
    "                x.append(word)\n",
    "                y.append(count)\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.barplot(x=y, y=x)\n",
    "        plt.show()\n",
    "\n",
    "    return vocab\n",
    "\n",
    "## showgraph set to True to visualize\n",
    "vocab= create_vocab(preprocessed, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e99dc8-920b-480d-8cbf-696102812cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEFINE LSTM MODEL #####\n",
    "from torch import nn\n",
    "\n",
    "# Define LSTM Model\n",
    "class LstmTextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, lstm_size, dense_size, output_size, lstm_layers=2, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.lstm_size = lstm_size\n",
    "    self.dense_size = dense_size\n",
    "    self.output_size = output_size\n",
    "    self.lstm_layers = lstm_layers\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if dense_size == 0:\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "    else:\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, output_size)\n",
    "\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state\n",
    "    \"\"\"\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "\n",
    "    return hidden\n",
    "\n",
    "  def forward(self, nn_input_text, hidden_state):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the model on nn_input\n",
    "    \"\"\"\n",
    "    batch_size = nn_input_text.size(0)\n",
    "    nn_input_text = nn_input_text.long()\n",
    "    embeds = self.embedding(nn_input_text)\n",
    "    lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "    # Stack up LSTM outputs, apply dropout\n",
    "    lstm_out = lstm_out[-1,:,:]\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if self.dense_size == 0:\n",
    "        out = self.fc(lstm_out)\n",
    "    else:\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        out = self.fc2(dense_out)\n",
    "    # Softmax\n",
    "    logps = self.softmax(out)\n",
    "\n",
    "    return logps, hidden_state\n",
    "      \n",
    "     \n",
    "# Define LSTM Tokenizer\n",
    "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "  '''\n",
    "  Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "  '''\n",
    "  X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "  for i, text in enumerate(X):\n",
    "    tokens = tokenize_text(text, 3) \n",
    "    token_ids = [vocab[word] for word in tokens]\n",
    "    end_idx = min(len(token_ids), seq_len)\n",
    "    if padding == 'right':\n",
    "      X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "    elif padding == 'left':\n",
    "      start_idx = max(seq_len - len(token_ids), 0)\n",
    "      X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "  return torch.tensor(X_tmp, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9e9ff-1a6f-4604-ba5c-dd6dd8daa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATASET CLASS DAN LOADER\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a DataSet Class which simply return (x, y) pair\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.datalist=[(x[i], y[i]) for i in range(len(y))]\n",
    "  def __len__(self):\n",
    "    return len(self.datalist)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.datalist[idx]\n",
    "      \n",
    "# Data Loader\n",
    "def create_data_loader(X, y, indices, batch_size, shuffle):\n",
    "  X_sampled = np.array(X, dtype=object)[indices]\n",
    "  y_sampled = np.array(y)[indices].astype(int)\n",
    "  dataset = SimpleDataset(X_sampled, y_sampled)\n",
    "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "  return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d283bd-fcb4-43be-89fd-9c5468bc86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLINC CYCLE DAN MATRICS UNTUK EVALUASI\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train_cycles(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "  result = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time', 'ms/text'], index=num_samples)\n",
    "\n",
    "  for n in num_samples:\n",
    "    print(\"\")\n",
    "    logger.info(\"############### Start training for %d samples ###############\" %n)\n",
    "\n",
    "    # Stratified sampling\n",
    "    train_size = n / len(y_all)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.2 , random_state=rand_seed)\n",
    "    train_indices, valid_indices = next(sss.split(X_all, y_all))\n",
    "\n",
    "    # Sample input data\n",
    "    train_loader = create_data_loader(X_all, y_all, train_indices, batch_size, True)\n",
    "    valid_loader = create_data_loader(X_all, y_all, valid_indices, batch_size, False)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "      model = LstmTextClassifier(len(vocab)+1, embed_size=512, lstm_size=1024, dense_size=0, output_size=5, lstm_layers=4, dropout=0.2)\n",
    "      model.embedding.weight.data.uniform_(-1, 1)\n",
    "    elif model_type == 'BERT':\n",
    "      model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "    start_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    acc, f1, model_trained = train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "    end_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    duration = end_time - start_time\n",
    "    logger.info(\"Process Time (sec): {}\".format(duration))\n",
    "    result.loc[n] = (round(acc,4), round(f1,4), duration, duration/n*1000)\n",
    "\n",
    "  return result, model_trained\n",
    "\n",
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  f1 = f1_score(y_true, y_pred, average='macro')\n",
    "  return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc431977-42d3-4835-bf6f-6831a2ba9dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
