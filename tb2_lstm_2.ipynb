{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483dd18-5509-4b0f-b808-5e5207d66bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ADOPTED FROM https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7328e6-f0c6-4189-8c32-458d9a3041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fd5b8-e49e-4c46-a145-2ee03af29d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE DAN BUAT LIST\n",
    "filepath = 'dataset_vaksin.csv'\n",
    "column_names = [\"Tweet\", \"Label\"]\n",
    "\n",
    "df = read_csv(filepath, header=0, sep=',', names=column_names)\n",
    "messages = df.Tweet.to_list()\n",
    "sentiments_ = df.Label.to_list()\n",
    "\n",
    "sentiments = [1 if sentiment =='Setuju Vaksin' else 0 for sentiment in sentiments_]\n",
    "\n",
    "#print(df)\n",
    "#print('----------------------------------------------------------------')\n",
    "print(messages[0:10])\n",
    "#print('----------------------------------------------------------------')\n",
    "print(sentiments[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8292df-eef9-481b-a132-7146a66cd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logger.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abdc79-b9aa-49cd-ba52-c3b26eaf6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VOCAB\n",
    "def create_vocab(messages, show_graph=False):\n",
    "    corpus = []\n",
    "    for message in tqdm(messages, desc=\"Tokenizing\"):\n",
    "        tokens = tokenize_text(message, 3) # Use option 3\n",
    "        corpus.extend(tokens)\n",
    "    logger.info(\"The number of all words: {}\".format(len(corpus)))\n",
    "\n",
    "    # Create Counter\n",
    "    counts = Counter(corpus)\n",
    "    logger.info(\"The number of unique words: {}\".format(len(counts)))\n",
    "\n",
    "    # Create BoW\n",
    "    bow = sorted(counts, key=counts.get, reverse=True)\n",
    "    logger.info(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
    "\n",
    "    # Indexing vocabrary, starting from 1.\n",
    "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    if show_graph:\n",
    "        from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "        # Generate Word Cloud image\n",
    "        text = \" \".join(corpus)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update([\"will\", \"report\", \"reporting\", \"market\", \"stock\", \"share\"])\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(text)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Show most frequent words in a bar graph\n",
    "        most = counts.most_common()[:80]\n",
    "        x, y = [], []\n",
    "        for word, count in most:\n",
    "            if word not in stopwords:\n",
    "                x.append(word)\n",
    "                y.append(count)\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.barplot(x=y, y=x)\n",
    "        plt.show()\n",
    "\n",
    "    return vocab\n",
    "\n",
    "#vocab= create_vocab(preprocessed, True)\n",
    "vocab= create_vocab(sentiments, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e99dc8-920b-480d-8cbf-696102812cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
