{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483dd18-5509-4b0f-b808-5e5207d66bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ADOPTED FROM https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7328e6-f0c6-4189-8c32-458d9a3041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__) #<<<<<<<<<<<<<<<<<<<<\n",
    "import time\n",
    "\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fd5b8-e49e-4c46-a145-2ee03af29d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE DAN BUAT LIST\n",
    "filepath = 'dataset_vaksin.csv'\n",
    "column_names = [\"Tweet\", \"Label\"]\n",
    "\n",
    "df = read_csv(filepath, header=0, sep=',', names=column_names)\n",
    "messages = df.Tweet.to_list()\n",
    "sentiments_ = df.Label.to_list()\n",
    "\n",
    "sentiments = [1 if sentiment =='Setuju Vaksin' else 0 for sentiment in sentiments_]\n",
    "\n",
    "#print(df)\n",
    "#print('----------------------------------------------------------------')\n",
    "print(messages[0:10])\n",
    "#print('----------------------------------------------------------------')\n",
    "print(sentiments[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e202a3-3bb7-47ea-bc1e-f737ce82f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS\n",
    "import re\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - removes any single character tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed\n",
    "    Returns\n",
    "    -------\n",
    "        text: The preprocessed text\n",
    "    \"\"\" \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    return text\n",
    "        \n",
    "# Process for all messages\n",
    "preprocessed = [preprocess(message) for message in tqdm(messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8292df-eef9-481b-a132-7146a66cd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logger.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abdc79-b9aa-49cd-ba52-c3b26eaf6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VOCAB\n",
    "def create_vocab(messages, show_graph=False):\n",
    "    corpus = []\n",
    "    for message in tqdm(messages, desc=\"Tokenizing\"):\n",
    "        ### PILIH OPTION DISINI ###\n",
    "        tokens = tokenize_text(message, 3) # Use option 3\n",
    "        corpus.extend(tokens)\n",
    "    logger.info(\"The number of all words: {}\".format(len(corpus)))\n",
    "\n",
    "    # Create Counter\n",
    "    counts = Counter(corpus)\n",
    "    logger.info(\"The number of unique words: {}\".format(len(counts)))\n",
    "\n",
    "    # Create BoW\n",
    "    bow = sorted(counts, key=counts.get, reverse=True)\n",
    "    logger.info(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
    "\n",
    "    # Indexing vocabrary, starting from 1.\n",
    "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    if show_graph:\n",
    "        from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "        # Generate Word Cloud image\n",
    "        text = \" \".join(corpus)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update([\"will\", \"report\", \"reporting\", \"market\", \"stock\", \"share\"])\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(text)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Show most frequent words in a bar graph\n",
    "        most = counts.most_common()[:80]\n",
    "        x, y = [], []\n",
    "        for word, count in most:\n",
    "            if word not in stopwords:\n",
    "                x.append(word)\n",
    "                y.append(count)\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.barplot(x=y, y=x)\n",
    "        plt.show()\n",
    "\n",
    "    return vocab\n",
    "\n",
    "## showgraph set to True to visualize\n",
    "vocab= create_vocab(preprocessed, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e99dc8-920b-480d-8cbf-696102812cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEFINE LSTM MODEL #####\n",
    "from torch import nn\n",
    "\n",
    "# Define LSTM Model\n",
    "class LstmTextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, lstm_size, dense_size, output_size, lstm_layers=2, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.lstm_size = lstm_size\n",
    "    self.dense_size = dense_size\n",
    "    self.output_size = output_size\n",
    "    self.lstm_layers = lstm_layers\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if dense_size == 0:\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "    else:\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, output_size)\n",
    "\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state\n",
    "    \"\"\"\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "\n",
    "    return hidden\n",
    "\n",
    "  def forward(self, nn_input_text, hidden_state):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the model on nn_input\n",
    "    \"\"\"\n",
    "    batch_size = nn_input_text.size(0)\n",
    "    nn_input_text = nn_input_text.long()\n",
    "    embeds = self.embedding(nn_input_text)\n",
    "    lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "    # Stack up LSTM outputs, apply dropout\n",
    "    lstm_out = lstm_out[-1,:,:]\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if self.dense_size == 0:\n",
    "        out = self.fc(lstm_out)\n",
    "    else:\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        out = self.fc2(dense_out)\n",
    "    # Softmax\n",
    "    logps = self.softmax(out)\n",
    "\n",
    "    return logps, hidden_state\n",
    "      \n",
    "     \n",
    "# Define LSTM Tokenizer\n",
    "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "  '''\n",
    "  Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "  '''\n",
    "  X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "  for i, text in enumerate(X):\n",
    "    tokens = tokenize_text(text, 3) \n",
    "    token_ids = [vocab[word] for word in tokens]\n",
    "    end_idx = min(len(token_ids), seq_len)\n",
    "    if padding == 'right':\n",
    "      X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "    elif padding == 'left':\n",
    "      start_idx = max(seq_len - len(token_ids), 0)\n",
    "      X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "  return torch.tensor(X_tmp, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9e9ff-1a6f-4604-ba5c-dd6dd8daa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATASET CLASS DAN LOADER\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a DataSet Class which simply return (x, y) pair\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.datalist=[(x[i], y[i]) for i in range(len(y))]\n",
    "  def __len__(self):\n",
    "    return len(self.datalist)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.datalist[idx]\n",
    "      \n",
    "# Data Loader\n",
    "def create_data_loader(X, y, indices, batch_size, shuffle):\n",
    "  X_sampled = np.array(X, dtype=object)[indices]\n",
    "  y_sampled = np.array(y)[indices].astype(int)\n",
    "  dataset = SimpleDataset(X_sampled, y_sampled)\n",
    "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "  return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d283bd-fcb4-43be-89fd-9c5468bc86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLINC CYCLE DAN MATRICS UNTUK EVALUASI\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "### ini tambahan\n",
    "rand_seed=None\n",
    "\n",
    "def train_cycles(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "  result = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time', 'ms/text'], index=num_samples)\n",
    "\n",
    "  for n in num_samples:\n",
    "    print(\"\")\n",
    "    logger.info(\"############### Start training for %d samples ###############\" %n)\n",
    "\n",
    "    # Stratified sampling\n",
    "    train_size = n / len(y_all)\n",
    "    ### cekvar\n",
    "    logger.info('Variable train_size = {}'.format(train_size))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.2 , random_state=rand_seed)\n",
    "    train_indices, valid_indices = next(sss.split(X_all, y_all))\n",
    "\n",
    "    ### cekvar\n",
    "    logger.info(X_all[0:20]);\n",
    "    logger.info(y_all[0:20]);\n",
    "    # Sample input data\n",
    "    train_loader = create_data_loader(X_all, y_all, train_indices, batch_size, True)\n",
    "    valid_loader = create_data_loader(X_all, y_all, valid_indices, batch_size, False)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "      model = LstmTextClassifier(len(vocab)+1, embed_size=512, lstm_size=1024, dense_size=0, output_size=5, lstm_layers=4, dropout=0.2)\n",
    "      model.embedding.weight.data.uniform_(-1, 1)\n",
    "    elif model_type == 'BERT':\n",
    "      model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "    start_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    acc, f1, model_trained = train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "    end_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    duration = end_time - start_time\n",
    "    logger.info(\"Process Time (sec): {}\".format(duration))\n",
    "    result.loc[n] = (round(acc,4), round(f1,4), duration, duration/n*1000)\n",
    "\n",
    "  return result, model_trained\n",
    "\n",
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  f1 = f1_score(y_true, y_pred, average='macro')\n",
    "  return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc431977-42d3-4835-bf6f-6831a2ba9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING MODEL\n",
    "from transformers import AdamW as AdamW_HF, get_linear_schedule_with_warmup\n",
    "\n",
    "def train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "    # Set variables\n",
    "###    logger = set_logger('sa_tweet_inperf', log_level)\n",
    "    num_total_opt_steps = int(len(train_loader) * epochs)\n",
    "    eval_every = len(train_loader) // 5\n",
    "###    eval_every = 5\n",
    "    warm_up_proportion = 0.1\n",
    "    logger.info('Total Training Steps: {} ({} batches x {} epochs)'.format(num_total_opt_steps, len(train_loader), epochs))\n",
    "## cek var\n",
    "    logger.info('Variable eval_every: {}'.format(eval_every))\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW_HF(model.parameters(), lr=lr, correct_bias=False) \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_total_opt_steps*warm_up_proportion, num_training_steps=num_total_opt_steps)  # PyTorch scheduler\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Set Train Mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialise\n",
    "    acc_train, f1_train, loss_train, acc_valid, f1_valid, loss_valid = [], [], [], [], [], []\n",
    "    best_f1, early_stop, steps = 0, 0, 0\n",
    "    ###class_names = ['0:Very Negative','1:Negative', '2:Neutral', '3:Positive', '4:Very Positive']\n",
    "    class_names = ['0:Negatif','1:Positif']\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "        logger.info('================     epoch {}     ==============='.format(epoch+1))\n",
    "\n",
    "        #################### Training ####################\n",
    "        # Initialise\n",
    "        loss_tmp, loss_cnt = 0, 0\n",
    "        y_pred_tmp, y_truth_tmp = [], []\n",
    "        hidden = model.init_hidden(batch_size) if model_type == \"LSTM\" else None\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            text_batch, labels = batch\n",
    "            # Skip the last batch of which size is not equal to batch_size\n",
    "            if labels.size(0) != batch_size:\n",
    "                break\n",
    "            steps += 1\n",
    "           \n",
    "            # Reset gradient\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialise after the previous training\n",
    "            if steps % eval_every == 1:\n",
    "                y_pred_tmp, y_truth_tmp = [], []\n",
    "\n",
    "            if model_type == \"LSTM\":\n",
    "                # Tokenize the input and move to device\n",
    "                text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "                hidden = tuple([each.data for each in hidden])\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "\n",
    "                # Get output and hidden state from the model, calculate the loss\n",
    "                logits, hidden = model(text_batch, hidden)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "            elif model_type == 'BERT':\n",
    "                # Tokenize the input and move to device\n",
    "                # Tokenizer Parameter\n",
    "                param_tk = {\n",
    "                    'return_tensors': \"pt\",\n",
    "                    'padding': 'max_length',\n",
    "                    'max_length': seq_len,\n",
    "                    'add_special_tokens': True,\n",
    "                    'truncation': True\n",
    "                }\n",
    "                text_batch = tokenizer_bert(text_batch, **param_tk).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                # Feedforward prediction\n",
    "                loss, logits = model(**text_batch, labels=labels)\n",
    "\n",
    "            y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
    "            y_truth_tmp.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Training Loss\n",
    "            loss_tmp += loss.item()\n",
    "            loss_cnt += 1\n",
    "\n",
    "            # Clip the gradient to prevent the exploading gradient problem in RNN/LSTM\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Update Weights and Learning Rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            #################### Evaluation ####################\n",
    "            if (steps % eval_every == 0) or ((steps % eval_every != 0) and (steps == len(train_loader))):\n",
    "                # Evaluate Training\n",
    "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
    "                acc_train.append(acc)\n",
    "                f1_train.append(f1)\n",
    "                loss_train.append(loss_tmp/loss_cnt)\n",
    "                loss_tmp, loss_cnt = 0, 0\n",
    "\n",
    "                # y_pred_tmp = np.zeros((len(y_valid), 5))\n",
    "                y_truth_tmp, y_pred_tmp = [], []\n",
    "\n",
    "                # Move to Evaluation Mode\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(valid_loader):\n",
    "                        text_batch, labels = batch\n",
    "                        # Skip the last batch of which size is not equal to batch_size\n",
    "                        if labels.size(0) != batch_size:\n",
    "                            break\n",
    "\n",
    "                        if model_type == \"LSTM\":\n",
    "                            # Tokenize the input and move to device\n",
    "                            text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
    "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                            # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "                            hidden = tuple([each.data for each in hidden])\n",
    "                            for each in hidden:\n",
    "                                each.to(device)\n",
    "\n",
    "                            # Get output and hidden state from the model, calculate the loss\n",
    "                            logits, hidden = model(text_batch, hidden)\n",
    "                            loss = criterion(logits, labels)\n",
    "                \n",
    "                        elif model_type == 'BERT':\n",
    "                            # Tokenize the input and move to device\n",
    "                            text_batch = tokenizer_bert(text_batch, **param_tk).to(device)\n",
    "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "                            # Feedforward prediction\n",
    "                            loss, logits = model(**text_batch, labels=labels)\n",
    "                    \n",
    "                        loss_tmp += loss.item()\n",
    "                        loss_cnt += 1\n",
    "\n",
    "                        y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
    "                        y_truth_tmp.extend(labels.cpu().numpy())\n",
    "                        # logger.debug('validation batch: {}, val_loss: {}'.format(i, loss.item() / len(valid_loader)))\n",
    "\n",
    "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
    "                logger.debug(\"Epoch: {}/{}, Step: {}, Loss: {:.4f}, Acc: {:.4f}, F1: {:.4f}\".format(epoch+1, epochs, steps, loss_tmp, acc, f1))\n",
    "                acc_valid.append(acc)\n",
    "                f1_valid.append(f1)\n",
    "                loss_valid.append(loss_tmp/loss_cnt)\n",
    "                loss_tmp, loss_cnt = 0, 0\n",
    "\n",
    "                # Back to train mode\n",
    "                model.train()\n",
    "\n",
    "        #################### End of each epoch ####################\n",
    "\n",
    "        # Show the last evaluation metrics\n",
    "        logger.info('Epoch: %d, Loss: %.4f, Acc: %.4f, F1: %.4f, LR: %.2e' % (epoch+1, loss_valid[-1], acc_valid[-1], f1_valid[-1], scheduler.get_last_lr()[0]))\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        y_truth_class = [class_names[int(idx)] for idx in y_truth_tmp]\n",
    "        y_predicted_class = [class_names[int(idx)] for idx in y_pred_tmp]\n",
    "        \n",
    "        titles_options = [(\"Actual Count\", None), (\"Normalised\", 'true')]\n",
    "        for title, normalize in titles_options:\n",
    "            disp = skplt.metrics.plot_confusion_matrix(y_truth_class, y_predicted_class, normalize=normalize, title=title, x_tick_rotation=75)\n",
    "        plt.show()\n",
    "\n",
    "        # plot training performance\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "        ax1.set_title(\"Losses\")\n",
    "        ax1.set_xlabel(\"Validation Cycle\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.plot(loss_train, 'b-o', label='Train Loss')\n",
    "        ax1.plot(loss_valid, 'r-o', label='Valid Loss')\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        \n",
    "        ax2.set_title(\"Evaluation\")\n",
    "        ax2.set_xlabel(\"Validation Cycle\")\n",
    "        ax2.set_ylabel(\"Score\")\n",
    "        ax2.set_ylim(0,1)\n",
    "        ax2.plot(acc_train, 'y-o', label='Accuracy (train)')\n",
    "        ax2.plot(f1_train, 'y--', label='F1 Score (train)')\n",
    "        ax2.plot(acc_valid, 'g-o', label='Accuracy (valid)')\n",
    "        ax2.plot(f1_valid, 'g--', label='F1 Score (valid)')\n",
    "        ax2.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # If improving, save the number. If not, count up for early stopping\n",
    "        if best_f1 < f1_valid[-1]:\n",
    "            early_stop = 0\n",
    "            best_f1 = f1_valid[-1]\n",
    "        else:\n",
    "            early_stop += 1\n",
    "\n",
    "        # Early stop if it reaches patience number\n",
    "        if early_stop >= patience:\n",
    "            break\n",
    "\n",
    "        # Prepare for the next epoch\n",
    "        if device == 'cuda:0':\n",
    "            torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "\n",
    "    return acc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981817d-9ce9-48d0-9cec-a11536d452cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN!!!\n",
    "# Define the training parameters\n",
    "num_samples = [100, 200, 400, 550, 700]\n",
    "epochs=5 # default 5\n",
    "patience=3\n",
    "batch_size=64\n",
    "seq_len = 30\n",
    "lr=3e-4\n",
    "clip=5\n",
    "log_level=logging.DEBUG\n",
    "\n",
    "# Run!\n",
    "###result_lstm, model_trained_lstm = train_cycles(train_df['text'], train_df['label'], vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "result_lstm, model_trained_lstm = train_cycles(preprocessed, sentiments, vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "\n",
    "# Save the model and show the result\n",
    "torch.save(model_trained_lstm.state_dict(), output_dir + 'stocktwit_lstm.dict')\n",
    "result_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2bf09-b1ae-44f4-b157-8325e6f98837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
