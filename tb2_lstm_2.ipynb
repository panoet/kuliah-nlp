{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0483dd18-5509-4b0f-b808-5e5207d66bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ADOPTED FROM https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7328e6-f0c6-4189-8c32-458d9a3041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__) #<<<<<<<<<<<<<<<<<<<<\n",
    "import time\n",
    "\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6fd5b8-e49e-4c46-a145-2ee03af29d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa kakk dah vaksin pulang mam minum obat darah dah darah', 'aaa tempat vaksin pulang', 'abis dpt vaksin sekolah ajar ptm ya nggak daring lg ya', 'abis vaksin', 'abis vaksin bawa ngantuk', 'abis vaksin bknny lmes mlh hyperactive curiga td disuntikny bantu dana bos', 'abis vaksin bole minum boba ga sih', 'abis vaksin cinta', 'abis vaksin dosis imun kuat bersin mulu duuh', 'abis vaksin gak titan']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# LOAD FILE DAN BUAT LIST\n",
    "filepath = 'dataset_vaksin.csv'\n",
    "column_names = [\"Tweet\", \"Label\"]\n",
    "\n",
    "df = read_csv(filepath, header=0, sep=',', names=column_names)\n",
    "messages = df.Tweet.to_list()\n",
    "sentiments_ = df.Label.to_list()\n",
    "\n",
    "sentiments = [1 if sentiment =='Setuju Vaksin' else 0 for sentiment in sentiments_]\n",
    "\n",
    "#print(df)\n",
    "#print('----------------------------------------------------------------')\n",
    "print(messages[0:10])\n",
    "#print('----------------------------------------------------------------')\n",
    "print(sentiments[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e202a3-3bb7-47ea-bc1e-f737ce82f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 721/721 [00:00<00:00, 68469.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESS\n",
    "import re\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - removes any single character tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed\n",
    "    Returns\n",
    "    -------\n",
    "        text: The preprocessed text\n",
    "    \"\"\" \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    return text\n",
    "        \n",
    "# Process for all messages\n",
    "preprocessed = [preprocess(message) for message in tqdm(messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8292df-eef9-481b-a132-7146a66cd051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logger.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4abdc79-b9aa-49cd-ba52-c3b26eaf6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 721/721 [00:00<00:00, 6364.70it/s]\n",
      "INFO:__main__:The number of all words: 7946\n",
      "INFO:__main__:The number of unique words: 2540\n",
      "INFO:__main__:Top 40 frequent words: ['vaksin', 'yg', 'udah', 'covid', 'ga', 'dosis', 'ya', 'anak', 'gue', 'vaksinasi', 'abis', 'aja', 'kali', 'kalo', 'gak', 'nya', 'dah', 'sinovac', 'maju', 'sertifikat', 'pas', 'aman', 'sakit', 'takut', 'gua', 'sehat', 'indonesia', 'udh', 'perintah', 'sih', 'baik', 'rakyat', 'lo', 'pimpin', 'masuk', 'habis', 'izin', 'jokowima', 'ruf', 'pake']\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCAB\n",
    "def create_vocab(messages, show_graph=False):\n",
    "    corpus = []\n",
    "    for message in tqdm(messages, desc=\"Tokenizing\"):\n",
    "        ### PILIH OPTION DISINI ###\n",
    "        tokens = tokenize_text(message, 3) # Use option 3\n",
    "        corpus.extend(tokens)\n",
    "    logger.info(\"The number of all words: {}\".format(len(corpus)))\n",
    "\n",
    "    # Create Counter\n",
    "    counts = Counter(corpus)\n",
    "    logger.info(\"The number of unique words: {}\".format(len(counts)))\n",
    "\n",
    "    # Create BoW\n",
    "    bow = sorted(counts, key=counts.get, reverse=True)\n",
    "    logger.info(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
    "\n",
    "    # Indexing vocabrary, starting from 1.\n",
    "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    if show_graph:\n",
    "        from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "        # Generate Word Cloud image\n",
    "        text = \" \".join(corpus)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update([\"will\", \"report\", \"reporting\", \"market\", \"stock\", \"share\"])\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(text)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Show most frequent words in a bar graph\n",
    "        most = counts.most_common()[:80]\n",
    "        x, y = [], []\n",
    "        for word, count in most:\n",
    "            if word not in stopwords:\n",
    "                x.append(word)\n",
    "                y.append(count)\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.barplot(x=y, y=x)\n",
    "        plt.show()\n",
    "\n",
    "    return vocab\n",
    "\n",
    "## showgraph set to True to visualize\n",
    "vocab= create_vocab(preprocessed, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e99dc8-920b-480d-8cbf-696102812cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEFINE LSTM MODEL #####\n",
    "from torch import nn\n",
    "\n",
    "# Define LSTM Model\n",
    "class LstmTextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, lstm_size, dense_size, output_size, lstm_layers=2, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.lstm_size = lstm_size\n",
    "    self.dense_size = dense_size\n",
    "    self.output_size = output_size\n",
    "    self.lstm_layers = lstm_layers\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if dense_size == 0:\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "    else:\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, output_size)\n",
    "\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state\n",
    "    \"\"\"\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "\n",
    "    return hidden\n",
    "\n",
    "  def forward(self, nn_input_text, hidden_state):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the model on nn_input\n",
    "    \"\"\"\n",
    "    batch_size = nn_input_text.size(0)\n",
    "    nn_input_text = nn_input_text.long()\n",
    "    embeds = self.embedding(nn_input_text)\n",
    "    lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "    # Stack up LSTM outputs, apply dropout\n",
    "    lstm_out = lstm_out[-1,:,:]\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if self.dense_size == 0:\n",
    "        out = self.fc(lstm_out)\n",
    "    else:\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        out = self.fc2(dense_out)\n",
    "    # Softmax\n",
    "    logps = self.softmax(out)\n",
    "\n",
    "    return logps, hidden_state\n",
    "      \n",
    "     \n",
    "# Define LSTM Tokenizer\n",
    "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "  '''\n",
    "  Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "  '''\n",
    "  X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "  for i, text in enumerate(X):\n",
    "    tokens = tokenize_text(text, 3) \n",
    "    token_ids = [vocab[word] for word in tokens]\n",
    "    end_idx = min(len(token_ids), seq_len)\n",
    "    if padding == 'right':\n",
    "      X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "    elif padding == 'left':\n",
    "      start_idx = max(seq_len - len(token_ids), 0)\n",
    "      X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "  return torch.tensor(X_tmp, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a9e9ff-1a6f-4604-ba5c-dd6dd8daa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATASET CLASS DAN LOADER\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a DataSet Class which simply return (x, y) pair\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.datalist=[(x[i], y[i]) for i in range(len(y))]\n",
    "  def __len__(self):\n",
    "    return len(self.datalist)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.datalist[idx]\n",
    "      \n",
    "# Data Loader\n",
    "def create_data_loader(X, y, indices, batch_size, shuffle):\n",
    "  X_sampled = np.array(X, dtype=object)[indices]\n",
    "  y_sampled = np.array(y)[indices].astype(int)\n",
    "  dataset = SimpleDataset(X_sampled, y_sampled)\n",
    "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "  return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d283bd-fcb4-43be-89fd-9c5468bc86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLINC CYCLE DAN MATRICS UNTUK EVALUASI\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "### ini tambahan\n",
    "rand_seed=None\n",
    "\n",
    "def train_cycles(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "  result = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time', 'ms/text'], index=num_samples)\n",
    "\n",
    "  for n in num_samples:\n",
    "    print(\"\")\n",
    "    logger.info(\"############### Start training for %d samples ###############\" %n)\n",
    "\n",
    "    # Stratified sampling\n",
    "    train_size = n / len(y_all)\n",
    "    ### cekvar\n",
    "    logger.info('Variable train_size = {}'.format(train_size))\n",
    "    ###sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.2 , random_state=rand_seed)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.3 , random_state=rand_seed)\n",
    "    train_indices, valid_indices = next(sss.split(X_all, y_all))\n",
    "\n",
    "    ### cekvar\n",
    "    logger.info(X_all[0:20])\n",
    "    logger.info(y_all[0:20])\n",
    "    logger.info(train_indices)\n",
    "    logger.info(batch_size)\n",
    "    # Sample input data\n",
    "    train_loader = create_data_loader(X_all, y_all, train_indices, batch_size, True)\n",
    "    valid_loader = create_data_loader(X_all, y_all, valid_indices, batch_size, False)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "      ###model = LstmTextClassifier(len(vocab)+1, embed_size=512, lstm_size=1024, dense_size=0, output_size=5, lstm_layers=4, dropout=0.2)\n",
    "      model = LstmTextClassifier(len(vocab)+1, embed_size=512, lstm_size=1024, dense_size=0, output_size=2, lstm_layers=4, dropout=0.2)\n",
    "      model.embedding.weight.data.uniform_(-1, 1)\n",
    "    elif model_type == 'BERT':\n",
    "      model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "    start_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    acc, f1, model_trained = train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "    end_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    duration = end_time - start_time\n",
    "    logger.info(\"Process Time (sec): {}\".format(duration))\n",
    "    result.loc[n] = (round(acc,4), round(f1,4), duration, duration/n*1000)\n",
    "\n",
    "  return result, model_trained\n",
    "\n",
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  f1 = f1_score(y_true, y_pred, average='macro')\n",
    "  return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fec5e6-5469-4bbe-8cbc-f81bfe957274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING MODEL\n",
    "from transformers import AdamW as AdamW_HF, get_linear_schedule_with_warmup\n",
    "\n",
    "def train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "    # Set variables\n",
    "    ###logger = set_logger('sa_tweet_inperf', log_level)\n",
    "    num_total_opt_steps = int(len(train_loader) * epochs)\n",
    "    ###kampret ini codenya beda versi\n",
    "    ###eval_every = len(train_loader) // 5\n",
    "    eval_every = len(train_loader) / 2\n",
    "    warm_up_proportion = 0.1\n",
    "    logger.info('Total Training Steps: {} ({} batches x {} epochs)'.format(num_total_opt_steps, len(train_loader), epochs))\n",
    "    ### cek var\n",
    "    logger.info('Variable eval_every: {}'.format(eval_every))\n",
    "    logger.info('Len train_loader: {}'.format(len(train_loader)))\n",
    "    logger.info('Len valid_loader: {}'.format(len(valid_loader)))\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW_HF(model.parameters(), lr=lr, correct_bias=False) \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_total_opt_steps*warm_up_proportion, num_training_steps=num_total_opt_steps)  # PyTorch scheduler\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Set Train Mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialise\n",
    "    acc_train, f1_train, loss_train, acc_valid, f1_valid, loss_valid = [], [], [], [], [], []\n",
    "    best_f1, early_stop, steps = 0, 0, 0\n",
    "    ###class_names = ['0:Very Negative','1:Negative', '2:Neutral', '3:Positive', '4:Very Positive']\n",
    "    class_names = ['0:Negatif','1:Positif']\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "        logger.info('================     epoch {}     ==============='.format(epoch+1))\n",
    "\n",
    "        #################### Training ####################\n",
    "        # Initialise\n",
    "        loss_tmp, loss_cnt = 0, 0\n",
    "        y_pred_tmp, y_truth_tmp = [], []\n",
    "        hidden = model.init_hidden(batch_size) if model_type == \"LSTM\" else None\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            text_batch, labels = batch\n",
    "            # Skip the last batch of which size is not equal to batch_size\n",
    "            if labels.size(0) != batch_size:\n",
    "                break\n",
    "            steps += 1\n",
    "           \n",
    "            # Reset gradient\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialise after the previous training\n",
    "            if steps % eval_every == 1:\n",
    "                y_pred_tmp, y_truth_tmp = [], []\n",
    "\n",
    "            if model_type == \"LSTM\":\n",
    "                # Tokenize the input and move to device\n",
    "                text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "                hidden = tuple([each.data for each in hidden])\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "\n",
    "                # Get output and hidden state from the model, calculate the loss\n",
    "                logits, hidden = model(text_batch, hidden)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "            elif model_type == 'BERT':\n",
    "                # Tokenize the input and move to device\n",
    "                # Tokenizer Parameter\n",
    "                param_tk = {\n",
    "                    'return_tensors': \"pt\",\n",
    "                    'padding': 'max_length',\n",
    "                    'max_length': seq_len,\n",
    "                    'add_special_tokens': True,\n",
    "                    'truncation': True\n",
    "                }\n",
    "                text_batch = tokenizer_bert(text_batch, **param_tk).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                # Feedforward prediction\n",
    "                loss, logits = model(**text_batch, labels=labels)\n",
    "\n",
    "            y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
    "            y_truth_tmp.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Training Loss\n",
    "            loss_tmp += loss.item()\n",
    "            loss_cnt += 1\n",
    "\n",
    "            # Clip the gradient to prevent the exploading gradient problem in RNN/LSTM\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Update Weights and Learning Rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            #################### Evaluation ####################\n",
    "            if (steps % eval_every == 0) or ((steps % eval_every != 0) and (steps == len(train_loader))):\n",
    "                # Evaluate Training\n",
    "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
    "                acc_train.append(acc)\n",
    "                f1_train.append(f1)\n",
    "                ###loss_train.append(loss_tmp/loss_cnt)\n",
    "                ### cek var\n",
    "                logger.info('Variable loss_tmp: {}'.format(loss_tmp))\n",
    "                logger.info('Variable loss_cnt: {}'.format(loss_cnt))\n",
    "                loss_train.append(loss_tmp/loss_cnt)\n",
    "                loss_tmp, loss_cnt = 0, 0\n",
    "\n",
    "                # y_pred_tmp = np.zeros((len(y_valid), 5))\n",
    "                y_truth_tmp, y_pred_tmp = [], []\n",
    "\n",
    "                # Move to Evaluation Mode\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(valid_loader):\n",
    "                        text_batch, labels = batch\n",
    "                        # Skip the last batch of which size is not equal to batch_size\n",
    "                        if labels.size(0) != batch_size:\n",
    "                            break\n",
    "\n",
    "                        if model_type == \"LSTM\":\n",
    "                            # Tokenize the input and move to device\n",
    "                            text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
    "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                            # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "                            hidden = tuple([each.data for each in hidden])\n",
    "                            for each in hidden:\n",
    "                                each.to(device)\n",
    "\n",
    "                            # Get output and hidden state from the model, calculate the loss\n",
    "                            logits, hidden = model(text_batch, hidden)\n",
    "                            loss = criterion(logits, labels)\n",
    "                \n",
    "                        elif model_type == 'BERT':\n",
    "                            # Tokenize the input and move to device\n",
    "                            text_batch = tokenizer_bert(text_batch, **param_tk).to(device)\n",
    "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "                            # Feedforward prediction\n",
    "                            loss, logits = model(**text_batch, labels=labels)\n",
    "                    \n",
    "                        loss_tmp += loss.item()\n",
    "                        loss_cnt += 1\n",
    "\n",
    "                        y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
    "                        y_truth_tmp.extend(labels.cpu().numpy())\n",
    "                        logger.debug('validation batch: {}, val_loss: {}'.format(i, loss.item() / len(valid_loader)))\n",
    "\n",
    "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
    "                logger.debug(\"Epoch: {}/{}, Step: {}, Loss: {:.4f}, Acc: {:.4f}, F1: {:.4f}\".format(epoch+1, epochs, steps, loss_tmp, acc, f1))\n",
    "                acc_valid.append(acc)\n",
    "                f1_valid.append(f1)\n",
    "                ###loss_valid.append(loss_tmp/loss_cnt)\n",
    "                ### cek var\n",
    "                logger.info('Variable loss_tmp: {}'.format(loss_tmp))\n",
    "                logger.info('Variable loss_cnt: {}'.format(loss_cnt))\n",
    "                loss_valid.append(loss_tmp/loss_cnt)\n",
    "                loss_tmp, loss_cnt = 0, 0\n",
    "\n",
    "                # Back to train mode\n",
    "                model.train()\n",
    "\n",
    "        #################### End of each epoch ####################\n",
    "\n",
    "        # Show the last evaluation metrics\n",
    "        logger.info(loss_valid)\n",
    "        logger.info('Epoch: %d, Loss: %.4f, Acc: %.4f, F1: %.4f, LR: %.2e' % (epoch+1, loss_valid[-1], acc_valid[-1], f1_valid[-1], scheduler.get_last_lr()[0]))\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        y_truth_class = [class_names[int(idx)] for idx in y_truth_tmp]\n",
    "        y_predicted_class = [class_names[int(idx)] for idx in y_pred_tmp]\n",
    "        \n",
    "        titles_options = [(\"Actual Count\", None), (\"Normalised\", 'true')]\n",
    "        for title, normalize in titles_options:\n",
    "            disp = skplt.metrics.plot_confusion_matrix(y_truth_class, y_predicted_class, normalize=normalize, title=title, x_tick_rotation=75)\n",
    "        plt.show()\n",
    "\n",
    "        # plot training performance\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "        ax1.set_title(\"Losses\")\n",
    "        ax1.set_xlabel(\"Validation Cycle\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.plot(loss_train, 'b-o', label='Train Loss')\n",
    "        ax1.plot(loss_valid, 'r-o', label='Valid Loss')\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        \n",
    "        ax2.set_title(\"Evaluation\")\n",
    "        ax2.set_xlabel(\"Validation Cycle\")\n",
    "        ax2.set_ylabel(\"Score\")\n",
    "        ax2.set_ylim(0,1)\n",
    "        ax2.plot(acc_train, 'y-o', label='Accuracy (train)')\n",
    "        ax2.plot(f1_train, 'y--', label='F1 Score (train)')\n",
    "        ax2.plot(acc_valid, 'g-o', label='Accuracy (valid)')\n",
    "        ax2.plot(f1_valid, 'g--', label='F1 Score (valid)')\n",
    "        ax2.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # If improving, save the number. If not, count up for early stopping\n",
    "        if best_f1 < f1_valid[-1]:\n",
    "            early_stop = 0\n",
    "            best_f1 = f1_valid[-1]\n",
    "        else:\n",
    "            early_stop += 1\n",
    "\n",
    "        # Early stop if it reaches patience number\n",
    "        if early_stop >= patience:\n",
    "            break\n",
    "\n",
    "        # Prepare for the next epoch\n",
    "        if device == 'cuda:0':\n",
    "            torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "\n",
    "    return acc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b981817d-9ce9-48d0-9cec-a11536d452cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:############### Start training for 100 samples ###############\n",
      "INFO:__main__:Variable train_size = 0.13869625520110956\n",
      "INFO:__main__:['aaa kakk dah vaksin pulang mam minum obat darah dah darah', 'aaa tempat vaksin pulang', 'abis dpt vaksin sekolah ajar ptm ya nggak daring lg ya', 'abis vaksin', 'abis vaksin bawa ngantuk', 'abis vaksin bknny lmes mlh hyperactive curiga td disuntikny bantu dana bos', 'abis vaksin bole minum boba ga sih', 'abis vaksin cinta', 'abis vaksin dosis imun kuat bersin mulu duuh', 'abis vaksin gak titan', 'abis vaksin gel banget dah tangan', 'abis vaksin lemes banget', 'abis vaksin masuk malem mending tidur yay or nay', 'abis vaksin trs rehat dlu yaa', 'abis vaksin udh ngerasa lemes yah', 'aceh tenggara tambah vaksin', 'adek gue daftar vaksin kampus jam sampe skrg antre krn acak ga no antri', 'admin maaf program vaksinasi sms besok tanggal november buka jam ya tgl okt vaksin dosis jadwal nya vaksin dosis besok tgl november tp diberitahukan tq', 'agen berangkat ya langsung cuman keluarga tau nih vaksin macem', 'ah anjir lupa donlot apk peduli ap gt bwat dta vaksin']\n",
      "INFO:__main__:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:__main__:[628 407 119 622 158 417 343 266 230 410 375 613 441 287 145  90 128 400\n",
      " 487 438 423  51 350 178 583 670  54 292 269 668 201 310 440 340 125 571\n",
      " 566 535 673 552 260 576 234 100 573  47  95 464 329  48 477 404 445 430\n",
      " 222 558 333 286 395 564 578 189 247 345  46 250 168 711 611 705 660 586\n",
      " 335 153 342 192 388 162 547 356 109 277 165 483 301 362 369  32 138  72\n",
      " 307 636 602 637 394  80 172 550 344   4]\n",
      "INFO:__main__:64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total Training Steps: 10 (2 batches x 5 epochs)\n",
      "INFO:__main__:Variable eval_every: 1.0\n",
      "INFO:__main__:Len train_loader: 2\n",
      "INFO:__main__:Len valid_loader: 1\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]INFO:__main__:================     epoch 1     ===============\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "INFO:__main__:Variable loss_tmp: 0.7155943512916565\n",
      "INFO:__main__:Variable loss_cnt: 1\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "INFO:__main__:Variable loss_tmp: 0\n",
      "INFO:__main__:Variable loss_cnt: 0\n",
      "Epoch:   0%|          | 0/5 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_192/654134669.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Run!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m###result_lstm, model_trained_lstm = train_cycles(train_df['text'], train_df['label'], vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresult_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trained_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cycles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Save the model and show the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_192/1974406321.py\u001b[0m in \u001b[0;36mtrain_cycles\u001b[0;34m(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use time.process_time() for CPU time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use time.process_time() for CPU time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_192/1101592141.py\u001b[0m in \u001b[0;36mtrain_nn_model\u001b[0;34m(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variable loss_tmp: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variable loss_cnt: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mloss_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tmp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mloss_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mloss_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# RUN!!!\n",
    "# Define the training parameters\n",
    "num_samples = [100, 200, 400, 550, 700]\n",
    "epochs=5 # default 5\n",
    "patience=3\n",
    "batch_size=64\n",
    "seq_len = 30\n",
    "lr=3e-4\n",
    "clip=5\n",
    "log_level=logging.DEBUG\n",
    "\n",
    "# Run!\n",
    "###result_lstm, model_trained_lstm = train_cycles(train_df['text'], train_df['label'], vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "result_lstm, model_trained_lstm = train_cycles(preprocessed, sentiments, vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "\n",
    "# Save the model and show the result\n",
    "torch.save(model_trained_lstm.state_dict(), output_dir + 'stocktwit_lstm.dict')\n",
    "result_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737788b-ce3d-40e3-ae27-b61ac790067f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
