{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483dd18-5509-4b0f-b808-5e5207d66bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ADOPTED FROM https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7328e6-f0c6-4189-8c32-458d9a3041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, TensorDataset# create Tensor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fd5b8-e49e-4c46-a145-2ee03af29d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE DAN BUAT LIST\n",
    "filepath = 'dataset_vaksin.csv'\n",
    "column_names = [\"Tweet\", \"Label\"]\n",
    "\n",
    "df = read_csv(filepath, header=0, sep=',', names=column_names)\n",
    "messages = df.Tweet.to_list()\n",
    "sentiments_ = df.Label.to_list()\n",
    "\n",
    "sentiments = [1 if sentiment =='Setuju Vaksin' else 0 for sentiment in sentiments_]\n",
    "\n",
    "#print(df)\n",
    "#print('----------------------------------------------------------------')\n",
    "print(messages[0:10])\n",
    "#print('----------------------------------------------------------------')\n",
    "print(sentiments[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8292df-eef9-481b-a132-7146a66cd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logger.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abdc79-b9aa-49cd-ba52-c3b26eaf6fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
