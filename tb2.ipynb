{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93427ff4-b033-4c2c-ae4c-f985e4e7f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ADOPTED FROM https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d292d-6f63-495e-83b9-472b6f061791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0356c8d-74e4-4a33-aef1-40db1686b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'dataset.csv'\n",
    "column_names = [\"Tweet\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1ff0a-c989-4ca7-bcce-71d7ca9685fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat list dengan isi tweet dan label dari dataset\n",
    "df = read_csv(filepath, header=0, sep=',', names=column_names)\n",
    "tweets = df.Tweet.to_list()\n",
    "labels = df.Label.to_list()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1fd74-c39e-49ad-9731-18c5b3734d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ = ' '.join(tweets)\n",
    "# buat list dengan isi semua kata yang ada di tweets\n",
    "words = words_.split()\n",
    "# hitung jumlah kata menggunakan class Counter\n",
    "count_words = Counter(words)\n",
    "words_count = len(count_words)\n",
    "sorted_words = count_words.most_common(words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04ce61-525b-4f01-9216-978b7fb1548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words\n",
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fece6-741f-45de-99f5-9abfb695c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "tweets_int = []\n",
    "for tweet in tweets:\n",
    "    r = [vocab_to_int[w] for w in tweet.split()]\n",
    "    tweets_int.append(r)\n",
    "print (tweets_int[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455e1c3-94dd-4a5d-ba20-3fb081d88589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize labels\n",
    "encoded_labels = [1 if label =='Setuju Vaksin' else 0 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fc7b2-7e79-4f9a-95c7-e91aafd129d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding / truncating remaining data\n",
    "def pad_features(tweets_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(tweets_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, tweet in enumerate(tweets_int):\n",
    "        tweet_len = len(tweet)\n",
    "        \n",
    "        if tweet_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-tweet_len))\n",
    "            new = zeroes+tweet\n",
    "        elif tweet_len > seq_length:\n",
    "            new = tweet[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "        \n",
    "    return features\n",
    "\n",
    "features = pad_features(tweets_int, 20)\n",
    "print(features[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c34a8d-c569-4b45-8fdb-1d194924ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train 80% validation 10% test 10%\n",
    "split_frac = 0.8\n",
    "train_x = features[0:int(split_frac*len(features))]\n",
    "train_y = encoded_labels[0:int(split_frac*len(features))]\n",
    "\n",
    "remaining_x = features[int(split_frac*len(features)):]\n",
    "remaining_y = encoded_labels[int(split_frac*len(features)):]\n",
    "\n",
    "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "\n",
    "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "test_y = remaining_y[int(len(remaining_y)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac785ca-c75c-4d49-867b-9d18044c74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))# dataloaders\n",
    "batch_size = 50# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ecd57-5aac-4a5d-ad4c-24a06564c0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
